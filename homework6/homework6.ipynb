{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4187a26a",
   "metadata": {},
   "source": [
    "# Task1:\n",
    "\n",
    "We discussed how we can formulate RL problems as an MDP. Describe any\n",
    "real-world application that can be formulated as an MDP. Describe the state space, action\n",
    "space, transition model, and rewards for that problem. You do not need to be precise in the\n",
    "description of the transition model and reward (no formula is needed). Qualitative description\n",
    "is enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65fb9b9",
   "metadata": {},
   "source": [
    "Reinforcement learning is the idea of having pseudo-intelligent agent take certain actions in its environment. Machine Learning always relies on some objective, so the agent’s objective is to maximize the reward, which is given based on the agent’s actions in the environment. This is closely related to the Markov Decision Problem, because typically the environment is represented as a Markov Decision Problem. An MDP is made of states, actions, state transition probabilities, and rewards. In fact, it is useful to represent Reinforcement Learning algorithms as MDP problems because many algorithms use dynamic programming techniques. \n",
    "\n",
    "MDP problems can certainly be applied to real world applications, as they are quite useful for determining efficient sequences of actions, when actions may not always be fully effective. A real-world application of MDP can be finding the optimal way to harvest animals. For example, say there’s a certain river with fish. When you fish, you can earn a certain amount of money, but it may impact the population of fish in subsequent seasons, affecting future opportunities. The idea is to find the most ideal sequence of actions that will maximize the reward (money) earned from fishing. \n",
    "\n",
    "For simplicity, we can define the states as: sparse, average, and dense (referring to the population density of an area). The environment can be in any of those three states. \n",
    "\n",
    "The agent, who wishes to maximize reward can take two actions: fish or don’t fish. When the agent fishes, they will fish up a certain proportion of the total population, which can be a parameter for the agent. \n",
    "\n",
    "The state transition is straightforward. When the agent fishes, there is a high probability that the environment will transition to a state with lower density. When the agent chooses not to fish, there is a high probability that the environment transitions to a state with higher density. \n",
    "\n",
    "The reward is simply the amount of money made from fishing a certain number of fish. \n",
    "\n",
    "The agent can simulate fishing to find the most optimal steps to maximize the reward, and the proportion of fish is also configurable. Since proportions fall in the range [0, 1], it is simple to re-test the MDP with various proportions, and find which one yields the max reward. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef9a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
