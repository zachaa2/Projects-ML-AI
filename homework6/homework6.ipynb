{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f103f01",
   "metadata": {},
   "source": [
    "# Task1:\n",
    "\n",
    "**We discussed how we can formulate RL problems as an MDP. Describe any\n",
    "real-world application that can be formulated as an MDP. Describe the state space, action\n",
    "space, transition model, and rewards for that problem. You do not need to be precise in the\n",
    "description of the transition model and reward (no formula is needed). Qualitative description\n",
    "is enough.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4545c2f",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "\n",
    "Reinforcement learning is the idea of having pseudo-intelligent agent take certain actions in its environment. Machine Learning always relies on some objective, so the agent’s objective is to maximize the reward, which is given based on the agent’s actions in the environment. This is closely related to the Markov Decision Problem, because typically the environment is represented as a Markov Decision Problem. An MDP is made of states, actions, state transition probabilities, and rewards. In fact, it is useful to represent Reinforcement Learning algorithms as MDP problems because many algorithms use dynamic programming techniques. \n",
    "\n",
    "\n",
    "MDP problems can certainly be applied to real world applications, as they are quite useful for determining efficient sequences of actions, when actions may not always be fully effective. A real-world application of MDP can be finding the optimal way to harvest animals. For example, say there’s a certain river with fish. When you fish, you can earn a certain amount of money, but it may impact the population of fish in subsequent seasons, affecting future opportunities. The idea is to find the most ideal sequence of actions that will maximize the reward (money) earned from fishing. \n",
    "\n",
    "**State Space**\n",
    "\n",
    "For simplicity, we can define the states as: sparse, average, and dense (referring to the population density of an area). The environment can be in any of those three states. \n",
    "\n",
    "**Action Space**\n",
    "\n",
    "The agent, who wishes to maximize reward can take two actions: fish or don’t fish. When the agent fishes, they will fish up a certain proportion of the total population, which can be a parameter for the agent. \n",
    "\n",
    "**State Transitions**\n",
    "\n",
    "The state transition is straightforward. When the agent fishes, there is a high probability that the environment will transition to a state with lower density. When the agent chooses not to fish, there is a high probability that the environment transitions to a state with higher density. \n",
    "\n",
    "**Rewards**\n",
    "\n",
    "The reward is simply the amount of money made from fishing a certain number of fish. \n",
    "\n",
    "\n",
    "The agent can simulate fishing to find the most optimal steps to maximize the reward, and the proportion of fish is also configurable. Since proportions fall in the range [0, 1], it is simple to re-test the MDP with various proportions, and find which one yields the max reward. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59faf107",
   "metadata": {},
   "source": [
    "# Task2:\n",
    "\n",
    "**RL is used in various sectors - Healthcare, recommender systems and trading\n",
    "are a few of those. Pick one of the three areas. Explain one of the problems in any of these\n",
    "domains that can be more effectively solved by reinforcement learning. Find an open-source\n",
    "project (if any) that has addressed this problem. Explain this project in detail.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e323cbcc",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "Reinforcement learning certainly has uses in various sectors. I looked at Reinforcement Learning’s applications in the financial sector. \n",
    "\n",
    "The paper I looked at is [this paper on Deep Reinforcement Learning in Financial Markets](https://arxiv.org/ftp/arxiv/papers/1907/1907.04373.pdf)\n",
    "\n",
    "**Paper Introduction**\n",
    "\n",
    "The idea is to develop trading signals that are  robust, consistently profitable, and able to capture the financial trading market. In this paper, they discuss a lot about modifying existing approaches, and developing techniques to capture market data effectively, for use by a model. I will focus specifically on their model of a financial MDP. \n",
    "\t\n",
    "When describing the MDP, they discuss the three main parts of an MDP/Agent problem – the state space, action space and reward space.\n",
    "\n",
    "**State Space Description**\n",
    "\n",
    "The state space is defined such that an agent will trade with one particular security and can buy/sell contracts of the security. The agent can buy anywhere from one to some fixed number of contracts. The agent also uses technical indicators to make decisions, which are outlined in the paper. The state space is divided into two main parts, the position state and the market features. The position state is a 3D vector of information about the number of contracts bought, sold and the corresponding profit or loss. The market features are the technical indicators, which are financial metrics that accurately describe the environment (the market). The definition of each indicator used can be found in the paper. \n",
    "\n",
    "**Action Space Description**\n",
    "\n",
    "The action space is represented as a single value, 0, 1 or 2. These represent Hold, Buy and Sell signals respectively. When at any state the agent will decide to perform and action. They also implemented transaction costs which affect the state position, to better represent reality.  The interesting thing to note from this section is that there is a zero-market impact hypothesis, which means that no agent’s action can ever be significant enough to impact market features. This simplifies the problem, as it means that taking actions will only affect the state position. \n",
    "\n",
    "**Reward Space Description**\n",
    "\n",
    "The reward space is defined by attempting to maximize both the short term and long-term returns. They use the information from the state position space in their calculations of reward. Specifically, the information on the profit/loss metric. Every time an action is taken, they store the immediate reward (profit/loss) as represented by the state’s position. They also implement long-term profit/loss by accumulating the profit/loss until the agent observes a change in position. A useful example can be seen in section 2.1.3 of the paper. \n",
    "\n",
    "Unfortunately, I could not find corresponding code to go along with the paper, but the paper includes pseudocode, detailed descriptions and testing/experimentation to convey and support their ideas. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbff4fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
