{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c91c1231",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33b5679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a81995",
   "metadata": {},
   "source": [
    "The Problem I chose to solve using Neural Networks is image classification. For this problem, I will be using the MNIST dataset, a popular and large dataset containing handwritten digits. Using the handwritten digits, I will develop a model to classify the digits for a number 0-9. The dataset contains roughly 70,000 images. There's 60k images for training and 10k images for testing.\n",
    "\n",
    "[Source](http://yann.lecun.com/exdb/mnist/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005bdf1",
   "metadata": {},
   "source": [
    "Creating the dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e32f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'keras.api._v2.keras.datasets.mnist' from 'C:\\\\Users\\\\Aaron\\\\anaconda3\\\\lib\\\\site-packages\\\\keras\\\\api\\\\_v2\\\\keras\\\\datasets\\\\mnist\\\\__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ce71a4",
   "metadata": {},
   "source": [
    "Loading the dataset and splitting into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2812a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ab5de64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape: (60000, 28, 28)\n",
      "Testing Set Shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set Shape:\", x_train.shape)\n",
    "print(\"Testing Set Shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ecacd3",
   "metadata": {},
   "source": [
    "# Task 1: Researching Tensorflow Docs and other Resources\n",
    "\n",
    "To implement a neural network to solve this problem, I decided to use the tensorflow/keras packages. BElow are some of the resources I used to learn more about the packages and about Neural Networks in general. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b807eac6",
   "metadata": {},
   "source": [
    "NN Layers: \n",
    "\n",
    "[Softmax Activation Function](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Softmax)\n",
    "\n",
    "[Dense Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)\n",
    "\n",
    "[Flatten Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten)\n",
    "\n",
    "Model:\n",
    "\n",
    "[Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)\n",
    "\n",
    "The sequential model just groups a stack of layers in a linear fashion. \n",
    "\n",
    "[keras.Model](https://keras.io/api/models/model/)\n",
    "\n",
    "You can create a custom class that inherits from this class, which will allow you to have greater control over the model.\n",
    "\n",
    "Some more useful TensorFlow docs pages:\n",
    "\n",
    "[AutoDiff](https://www.tensorflow.org/guide/autodiff#gradients_with_respect_to_a_model)\n",
    "\n",
    "[GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape#used-in-the-notebooks)\n",
    "\n",
    "[Customizing Fit Function](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74794fb4",
   "metadata": {},
   "source": [
    "# Task 2: EDA and Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15bf068",
   "metadata": {},
   "source": [
    "We will first explore the data and visualize it, then build and compile the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4215a909",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7912c1b6",
   "metadata": {},
   "source": [
    "Let's try and show what some of the images in the training set look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39c0c494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(x_train[i], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7ca857",
   "metadata": {},
   "source": [
    "Let's see the unique values in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dab453e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf4a496",
   "metadata": {},
   "source": [
    "The unique values are all integers ranging from 0-255. This makes sense, as images are typically represented by color values ranging from 0-255. \n",
    "\n",
    "What about the target labels?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4911f767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75992b62",
   "metadata": {},
   "source": [
    "The target labels for our problem are integers from 0-9. This is what we are trying to predict with our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a144444b",
   "metadata": {},
   "source": [
    "We can also try and plot the target labels as a histogram to see the distribution over the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e78e5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5923., 6742., 5958., 6131., 5842., 5421., 5918., 6265., 5851.,\n",
       "        5949.]),\n",
       " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0d3b70",
   "metadata": {},
   "source": [
    "There appears to be a roughly uniform distribution over all possible target labels in the training set. This is good, as it means the model will have roughly the same aound of data to learn from for all target labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a2fb7",
   "metadata": {},
   "source": [
    "### Creating the Sequential Model Structure\n",
    "\n",
    "We can create an instance of the Sequential model class which will group network layers in a stack. The first layer will be to flatten the data, which will reduce the dimensionality of the data. Optionally, you could flatten the data using the reshape operation. \n",
    "\n",
    "I will add Dense layers, which are the standard densly connected neural network layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3fcbf8",
   "metadata": {},
   "source": [
    "Below is a custom Classback class. This is passed into the model when fitting the model to the data. This will aloow us to view and/or save information while the training step is in process. By overriding the on_epoch_end, we can view information about the model when an epoch finishes. \n",
    "\n",
    "This callback class is simple. It will just print the weights matrix and the bias vector after each epoch. Seeing the updated trainable variables as well as the loss at each epoch should be an empirical indication of the convergance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96f94196",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        print(\"\\nEpoch:\", epoch, \"\\nWeights and Bias: \\n\", model.trainable_variables)\n",
    "    \n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "\n",
    "my_callback = myCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e494f70e",
   "metadata": {},
   "source": [
    "Now I will establish the structure of the model. The model will be built using the keras Sequential API. I firsst add a Flatten layer which will change the input shape from (28,28) to (,784)\n",
    "\n",
    "Next there's two hidden dense layers, using the ReLU activation function. The final dense layer will have a shape of 10, because we have 10 possible target labels to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "268bf22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    # layer to take input and flatten the shape\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    \n",
    "    # two dense hidden layers with ReLU activation function\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    \n",
    "    # final layer of shape (,10) \n",
    "    keras.layers.Dense(10)  \n",
    "    \n",
    "])\n",
    "model.summary()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ec013",
   "metadata": {},
   "source": [
    "Now we can assign a loss function and a gradient descent algorithm to our model. For the loss function, I use Spare Categorical Crossentrpy since we are dealing with categorical data. The optimizer will be the ADAM optimizer (Adapotive Moment Estimation)\n",
    "\n",
    "See section 4.6 of [this paper](https://arxiv.org/pdf/1609.04747.pdf) for info on ADAM optimized alogrithm. ADAM uses alearning rate, which is an imporant tuning parameter. I initially choose a rate of 1e-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eb86566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I set from_logits=True becuase there is no Softmax layer to normalize the last output of the NN\n",
    "# This can be done by the loss function\n",
    "loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optim = keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f144535",
   "metadata": {},
   "source": [
    "Now that we've established the model structure, loss function and gradient descent alogrithm, we can compile the model with these parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f81de1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optim, loss=loss_function, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995277a5",
   "metadata": {},
   "source": [
    "This is the train step. The accuracy of the train-dev set and the loss gets automatically printed, and the callback object also prints the weights and bias at each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29575d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1850/1875 [============================>.] - ETA: 0s - loss: 1.7340 - accuracy: 0.8763\n",
      "Epoch: 0 \n",
      "Weights and Bias: \n",
      " [<tf.Variable 'dense/kernel:0' shape=(784, 128) dtype=float32, numpy=\n",
      "array([[-0.0290429 , -0.07434016,  0.05383185, ...,  0.06791159,\n",
      "        -0.07406383, -0.00083948],\n",
      "       [-0.00341306, -0.04872555,  0.07632641, ..., -0.00431782,\n",
      "         0.0748656 , -0.06475297],\n",
      "       [-0.04258145, -0.03561103, -0.06391864, ...,  0.07608018,\n",
      "         0.02269492, -0.08061688],\n",
      "       ...,\n",
      "       [-0.07865026, -0.07112232, -0.06028879, ..., -0.05582137,\n",
      "        -0.05563801,  0.05940155],\n",
      "       [-0.05566164,  0.04676985, -0.02946233, ...,  0.05205358,\n",
      "        -0.01026798, -0.03355665],\n",
      "       [-0.06728154, -0.05630093, -0.06812032, ..., -0.03156614,\n",
      "         0.04599058, -0.06836499]], dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([-0.01990358, -0.01392844, -0.00866197, -0.02223401, -0.02158568,\n",
      "       -0.00622281, -0.00038687, -0.03239794, -0.0029563 ,  0.00798397,\n",
      "        0.00414821, -0.00818664, -0.01984901, -0.01449372, -0.0030331 ,\n",
      "       -0.01610287, -0.00892901, -0.01399823, -0.00047904, -0.02504591,\n",
      "       -0.0159359 , -0.02714437, -0.02662092, -0.02234868, -0.01320638,\n",
      "       -0.01563989, -0.03391603, -0.02022113, -0.01115007,  0.00227652,\n",
      "       -0.01689444, -0.01978132, -0.01143486, -0.00776915, -0.00673209,\n",
      "       -0.02240446, -0.01360987, -0.03704994, -0.02513293, -0.01555846,\n",
      "        0.0049267 , -0.028076  , -0.01834617, -0.00974598, -0.01262883,\n",
      "       -0.01952297, -0.01828196, -0.0103985 , -0.00469401, -0.01565722,\n",
      "       -0.01628567, -0.0061431 , -0.01084141, -0.01696512, -0.00612005,\n",
      "       -0.0044524 , -0.02155747, -0.00102928, -0.01063406, -0.00888027,\n",
      "       -0.01424615, -0.01727133, -0.01496241, -0.01840798, -0.011798  ,\n",
      "       -0.00943515, -0.02177927, -0.02235302, -0.02034862, -0.02076966,\n",
      "       -0.02053669, -0.00210085, -0.01736986, -0.03064338, -0.01913788,\n",
      "       -0.02182895,  0.01240402, -0.0395823 , -0.01800179, -0.02640069,\n",
      "       -0.01624332, -0.01108157, -0.01416207, -0.01024751, -0.0163622 ,\n",
      "       -0.01323996, -0.03794217, -0.01610785, -0.01501829, -0.03236048,\n",
      "       -0.00665244, -0.0112114 , -0.00675111, -0.02332063,  0.00057297,\n",
      "       -0.02663895, -0.0181491 , -0.01793961, -0.00951856, -0.01341729,\n",
      "       -0.02594841, -0.01199735, -0.02009856, -0.01490595, -0.00021159,\n",
      "       -0.01265488, -0.02005532, -0.01700673, -0.02915148, -0.01516985,\n",
      "       -0.01021951, -0.02216548, -0.02482697, -0.02601692, -0.0135404 ,\n",
      "       -0.02657652, -0.02795261, -0.01357624, -0.03281244, -0.02302685,\n",
      "       -0.01173993, -0.02965672, -0.02502541, -0.03513929, -0.01060366,\n",
      "       -0.02434883, -0.01649923, -0.02408637], dtype=float32)>, <tf.Variable 'dense_1/kernel:0' shape=(128, 128) dtype=float32, numpy=\n",
      "array([[-0.11179835,  0.13161577,  0.02987692, ...,  0.06616518,\n",
      "        -0.0457485 , -0.14319293],\n",
      "       [-0.10538325, -0.07919795, -0.14222872, ..., -0.12127265,\n",
      "        -0.14098379, -0.01618835],\n",
      "       [ 0.03019987,  0.02926166, -0.12627384, ..., -0.06389613,\n",
      "        -0.08921184,  0.06247218],\n",
      "       ...,\n",
      "       [-0.05917039, -0.1208215 ,  0.11463039, ..., -0.1090778 ,\n",
      "         0.06518599, -0.03541034],\n",
      "       [-0.01342067, -0.15067865, -0.07560899, ..., -0.14951022,\n",
      "         0.02807274,  0.03662366],\n",
      "       [ 0.05154084,  0.05849626, -0.06466574, ...,  0.13438807,\n",
      "        -0.00719374,  0.08405656]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([-0.0193683 , -0.05765457, -0.03857868, -0.08133087,  0.01700776,\n",
      "       -0.00630795,  0.00308582, -0.02890208,  0.00169606, -0.00374766,\n",
      "       -0.0672524 , -0.02308327, -0.06514034, -0.04123346, -0.0658401 ,\n",
      "       -0.05184912, -0.05946545, -0.01653068, -0.03281264, -0.0088073 ,\n",
      "       -0.04283309, -0.01844819, -0.01411504, -0.02365463, -0.06520259,\n",
      "        0.01702346, -0.00874123, -0.09306938, -0.08146636,  0.00810078,\n",
      "       -0.0071696 , -0.03581421, -0.0490399 , -0.03742089, -0.06639355,\n",
      "       -0.06705557, -0.09281721, -0.02003269, -0.02392576, -0.07817177,\n",
      "       -0.00787737, -0.06093718, -0.05788609, -0.00652696, -0.02720863,\n",
      "       -0.0287349 , -0.02756721, -0.02229772, -0.06242992, -0.08142133,\n",
      "       -0.06496983, -0.0321993 , -0.0007523 , -0.05808578,  0.00457501,\n",
      "       -0.04494612, -0.06714834, -0.05382237, -0.03050168, -0.07609168,\n",
      "       -0.00490631,  0.01140983, -0.03174944, -0.06873906, -0.1051852 ,\n",
      "       -0.07242437, -0.06629679, -0.04761648, -0.03695058, -0.02232984,\n",
      "       -0.07346705, -0.02975113, -0.0346918 , -0.01324487, -0.00743033,\n",
      "       -0.0692656 , -0.04183357, -0.05591641, -0.0271737 , -0.03513808,\n",
      "       -0.00917026, -0.03489363, -0.12184739,  0.00503433, -0.05065355,\n",
      "       -0.01987695,  0.00434776, -0.03445778, -0.00968629, -0.07737146,\n",
      "       -0.00805494, -0.01603583, -0.00839719, -0.04770768, -0.06750902,\n",
      "       -0.0147569 , -0.03331865, -0.06713116, -0.05559325, -0.1013428 ,\n",
      "       -0.06499324, -0.05301942, -0.00537379, -0.04402861, -0.06587595,\n",
      "       -0.06522955, -0.0574874 , -0.01890237, -0.08732349, -0.06616586,\n",
      "       -0.01719279, -0.04137686, -0.00352951, -0.08483244, -0.06222016,\n",
      "       -0.01250368, -0.0441012 , -0.01171628, -0.04322024, -0.038447  ,\n",
      "       -0.05703557, -0.02896992, -0.05179442, -0.03374235, -0.02702136,\n",
      "       -0.0017462 , -0.09043871, -0.04015108], dtype=float32)>, <tf.Variable 'dense_2/kernel:0' shape=(128, 10) dtype=float32, numpy=\n",
      "array([[-0.12425896,  0.19688126, -0.06435505, ...,  0.13501683,\n",
      "         0.09007193,  0.08152125],\n",
      "       [-0.08035357, -0.06662166, -0.09106815, ...,  0.11000908,\n",
      "         0.16401105, -0.1079717 ],\n",
      "       [-0.10862604, -0.05581828,  0.07899289, ..., -0.09564263,\n",
      "         0.04261788, -0.11939279],\n",
      "       ...,\n",
      "       [-0.00385873, -0.06167296,  0.11017707, ..., -0.10102308,\n",
      "         0.00148399, -0.18581638],\n",
      "       [-0.09057549,  0.16819698,  0.20052738, ..., -0.03821959,\n",
      "        -0.17646347,  0.1440792 ],\n",
      "       [ 0.02449934, -0.13604687, -0.10758197, ..., -0.0805341 ,\n",
      "        -0.10616361,  0.15572861]], dtype=float32)>, <tf.Variable 'dense_2/bias:0' shape=(10,) dtype=float32, numpy=\n",
      "array([-0.0127704 , -0.00636422, -0.01654453, -0.0047969 ,  0.01357609,\n",
      "       -0.01670466,  0.0080326 ,  0.01337084,  0.02005448,  0.00159348],\n",
      "      dtype=float32)>]\n",
      "1875/1875 [==============================] - 2s 900us/step - loss: 1.7166 - accuracy: 0.8767\n",
      "Epoch 2/3\n",
      "1854/1875 [============================>.] - ETA: 0s - loss: 0.3851 - accuracy: 0.9200\n",
      "Epoch: 1 \n",
      "Weights and Bias: \n",
      " [<tf.Variable 'dense/kernel:0' shape=(784, 128) dtype=float32, numpy=\n",
      "array([[-0.0290429 , -0.07434016,  0.05383185, ...,  0.06791159,\n",
      "        -0.07406383, -0.00083948],\n",
      "       [-0.00341306, -0.04872555,  0.07632641, ..., -0.00431782,\n",
      "         0.0748656 , -0.06475297],\n",
      "       [-0.04258145, -0.03561103, -0.06391864, ...,  0.07608018,\n",
      "         0.02269492, -0.08061688],\n",
      "       ...,\n",
      "       [-0.07865026, -0.07112232, -0.06028879, ..., -0.05582137,\n",
      "        -0.05563801,  0.05940155],\n",
      "       [-0.05566164,  0.04676985, -0.02946233, ...,  0.05205358,\n",
      "        -0.01026798, -0.03355665],\n",
      "       [-0.06728154, -0.05630093, -0.06812032, ..., -0.03156614,\n",
      "         0.04599058, -0.06836499]], dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([-2.18445919e-02, -6.07562438e-03, -2.08002180e-02, -2.02510562e-02,\n",
      "       -1.60928108e-02, -1.22800628e-02, -5.80538390e-03, -3.89464907e-02,\n",
      "       -4.60314471e-03,  6.30683592e-03, -7.55346660e-03, -4.93346201e-03,\n",
      "       -1.99758615e-02, -3.73543985e-02,  3.23161250e-03, -1.09216813e-02,\n",
      "       -4.22236975e-03, -1.45466812e-02,  1.23797385e-02, -9.53688566e-03,\n",
      "       -2.58430503e-02, -2.71442924e-02, -2.83259787e-02, -1.49493124e-02,\n",
      "       -1.43318344e-02, -2.36004200e-02, -3.25994864e-02, -2.79092882e-02,\n",
      "       -2.53798459e-02,  7.02094170e-04, -2.05772929e-02, -3.20332870e-02,\n",
      "       -1.07501065e-02, -2.61967769e-03, -1.65181998e-02, -2.20038481e-02,\n",
      "       -2.08472461e-02, -4.20657694e-02, -2.27280036e-02, -2.69364547e-02,\n",
      "        1.44676305e-02, -4.59630974e-02, -2.05089860e-02, -1.03383409e-02,\n",
      "       -8.21130257e-03, -2.37727817e-02, -2.45333686e-02, -1.17793987e-02,\n",
      "       -1.28769670e-02, -1.99707467e-02, -3.09847035e-02, -8.16513132e-03,\n",
      "       -2.83859596e-02, -1.95600223e-02, -6.82777585e-03, -6.01684116e-03,\n",
      "       -2.59628072e-02, -2.76129485e-05, -1.22479545e-02, -1.65357720e-02,\n",
      "       -2.06768587e-02, -3.28917578e-02, -2.18306147e-02, -1.70091949e-02,\n",
      "       -1.28746200e-02, -1.10375686e-02, -1.91991739e-02, -1.71920881e-02,\n",
      "       -2.54620425e-02, -2.07696613e-02, -3.92509177e-02, -1.44932233e-02,\n",
      "       -1.89124625e-02, -2.28144564e-02, -3.00979093e-02, -4.09943387e-02,\n",
      "        8.52884725e-03, -5.34672141e-02, -1.89073943e-02, -4.40234803e-02,\n",
      "       -1.95007510e-02, -1.50594125e-02, -1.81055330e-02, -1.22468416e-02,\n",
      "       -2.56981775e-02, -2.14782618e-02, -4.03512381e-02, -2.34472696e-02,\n",
      "       -1.32604847e-02, -3.99759561e-02, -5.72339864e-03, -1.65087041e-02,\n",
      "       -6.46857871e-03, -2.58306265e-02,  3.01416535e-02, -3.34486067e-02,\n",
      "       -2.32723430e-02, -1.97673216e-02, -1.09938150e-02, -1.45430956e-02,\n",
      "       -2.71973964e-02, -1.46065038e-02, -1.70979220e-02, -3.16630937e-02,\n",
      "       -3.27549851e-03, -1.63530856e-02, -4.34818156e-02, -1.83361992e-02,\n",
      "       -3.21191885e-02, -1.43356118e-02, -4.94785700e-03, -1.53337698e-02,\n",
      "       -2.52173804e-02, -3.16914469e-02, -2.75307484e-02, -3.54979075e-02,\n",
      "       -3.49893980e-02, -9.25080664e-03, -4.03633788e-02, -2.67572999e-02,\n",
      "       -2.84920651e-02, -3.44360210e-02, -2.37572156e-02, -3.71298678e-02,\n",
      "       -1.81472246e-02, -1.48196109e-02, -1.50513500e-02, -3.24200615e-02],\n",
      "      dtype=float32)>, <tf.Variable 'dense_1/kernel:0' shape=(128, 128) dtype=float32, numpy=\n",
      "array([[-0.11169388,  0.1314499 ,  0.02987692, ...,  0.06740674,\n",
      "        -0.0457485 , -0.14289938],\n",
      "       [-0.12776221, -0.06175535, -0.13599451, ..., -0.11855701,\n",
      "        -0.1692864 , -0.01977451],\n",
      "       [ 0.02507611,  0.0167683 , -0.13221562, ..., -0.04421621,\n",
      "        -0.08921184,  0.08283973],\n",
      "       ...,\n",
      "       [-0.0602791 , -0.11827361,  0.11489159, ..., -0.13163942,\n",
      "         0.06557038, -0.0333278 ],\n",
      "       [-0.04120081, -0.15197022, -0.08811666, ..., -0.17565413,\n",
      "         0.02759196,  0.00994141],\n",
      "       [ 0.06277055,  0.05079066, -0.06491231, ...,  0.11054538,\n",
      "        -0.0067812 ,  0.09631796]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([-9.96300131e-02, -8.99557546e-02, -9.39490423e-02, -1.07739963e-01,\n",
      "        4.20720084e-03,  1.75167836e-04,  1.21958666e-02, -8.55740458e-02,\n",
      "       -1.69791840e-02, -3.57110314e-02, -1.32455438e-01, -4.46515791e-02,\n",
      "       -8.24488401e-02, -9.64943841e-02, -1.10981740e-01, -8.17445815e-02,\n",
      "       -1.05326712e-01, -4.83647510e-02, -8.76895636e-02, -3.67713422e-02,\n",
      "       -5.12541980e-02, -3.88463773e-02, -3.57967727e-02, -4.99322563e-02,\n",
      "       -8.73871073e-02,  1.42741231e-02, -2.57786103e-02, -1.22780696e-01,\n",
      "       -1.17404632e-01,  4.35293419e-03, -3.56568135e-02, -1.05661407e-01,\n",
      "       -1.00976288e-01, -8.48983750e-02, -1.47906974e-01, -1.16078816e-01,\n",
      "       -1.45798564e-01, -3.51497307e-02, -5.51482663e-02, -1.41987205e-01,\n",
      "       -1.75362788e-02, -1.33931652e-01, -4.66175489e-02, -2.00706404e-02,\n",
      "       -7.40188882e-02, -6.13142215e-02, -8.65151212e-02, -4.55609374e-02,\n",
      "       -1.29163623e-01, -1.06020689e-01, -8.80517960e-02, -4.72564511e-02,\n",
      "       -3.27595808e-02, -9.67367217e-02,  1.40019534e-02, -4.83512208e-02,\n",
      "       -9.79145244e-02, -9.57042426e-02, -4.13903855e-02, -1.45695001e-01,\n",
      "       -1.03445631e-02, -3.99025111e-03, -5.36547080e-02, -1.14711531e-01,\n",
      "       -1.40222102e-01, -9.34035406e-02, -1.08382910e-01, -8.81168023e-02,\n",
      "       -6.16264641e-02, -3.91809717e-02, -9.55241323e-02, -5.00624031e-02,\n",
      "       -5.39665334e-02, -6.43875971e-02, -4.76834029e-02, -1.07317135e-01,\n",
      "       -4.05281037e-02, -8.45562965e-02, -4.92322370e-02, -6.66632205e-02,\n",
      "       -6.25207052e-02, -8.74023661e-02, -1.89475685e-01,  3.91907692e-02,\n",
      "       -6.08946308e-02, -7.26849660e-02, -4.72494308e-03, -1.85924787e-02,\n",
      "       -4.94819283e-02, -1.21821605e-01, -2.86732875e-02, -4.75348793e-02,\n",
      "       -3.98630314e-02, -1.05645567e-01, -8.49966705e-02, -5.79939783e-02,\n",
      "       -7.97783360e-02, -1.15354039e-01, -1.05678111e-01, -1.30618468e-01,\n",
      "       -1.05967812e-01, -1.01042978e-01, -3.21954265e-02, -6.91536814e-02,\n",
      "       -1.01717457e-01, -8.28585476e-02, -1.03758737e-01, -7.59149045e-02,\n",
      "       -1.40347093e-01, -6.87331334e-02, -2.88424529e-02, -6.38717711e-02,\n",
      "       -1.16027305e-02, -1.42147884e-01, -9.65085626e-02, -1.47956191e-02,\n",
      "       -7.13320896e-02, -3.19781974e-02, -7.20001832e-02, -5.22590280e-02,\n",
      "       -9.30387005e-02, -5.02499156e-02, -1.27665669e-01, -6.12475015e-02,\n",
      "       -4.70058583e-02, -2.80655138e-02, -5.22479862e-02, -7.05744177e-02],\n",
      "      dtype=float32)>, <tf.Variable 'dense_2/kernel:0' shape=(128, 10) dtype=float32, numpy=\n",
      "array([[-0.10674836,  0.19241731, -0.03838589, ...,  0.11003049,\n",
      "         0.08906691,  0.09234355],\n",
      "       [-0.0559057 , -0.04436271, -0.05471472, ...,  0.06780189,\n",
      "         0.12598506, -0.07837351],\n",
      "       [-0.06572284, -0.02786732,  0.0577385 , ..., -0.09279996,\n",
      "         0.03294563, -0.11943477],\n",
      "       ...,\n",
      "       [-0.0127996 , -0.0630147 ,  0.09019623, ..., -0.1026158 ,\n",
      "         0.00896122, -0.19668554],\n",
      "       [-0.1243268 ,  0.1325807 ,  0.20683923, ...,  0.03530401,\n",
      "        -0.19039351,  0.11611439],\n",
      "       [ 0.00925033, -0.12392084, -0.10175513, ..., -0.06108075,\n",
      "        -0.08597632,  0.12948292]], dtype=float32)>, <tf.Variable 'dense_2/bias:0' shape=(10,) dtype=float32, numpy=\n",
      "array([-0.03085447,  0.00691964, -0.03865593, -0.00523042,  0.01732922,\n",
      "       -0.04181726, -0.02837104,  0.00933219,  0.07844578,  0.01452551],\n",
      "      dtype=float32)>]\n",
      "1875/1875 [==============================] - 2s 853us/step - loss: 0.3852 - accuracy: 0.9198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n",
      "1860/1875 [============================>.] - ETA: 0s - loss: 0.2659 - accuracy: 0.9367\n",
      "Epoch: 2 \n",
      "Weights and Bias: \n",
      " [<tf.Variable 'dense/kernel:0' shape=(784, 128) dtype=float32, numpy=\n",
      "array([[-0.0290429 , -0.07434016,  0.05383185, ...,  0.06791159,\n",
      "        -0.07406383, -0.00083948],\n",
      "       [-0.00341306, -0.04872555,  0.07632641, ..., -0.00431782,\n",
      "         0.0748656 , -0.06475297],\n",
      "       [-0.04258145, -0.03561103, -0.06391864, ...,  0.07608018,\n",
      "         0.02269492, -0.08061688],\n",
      "       ...,\n",
      "       [-0.07865026, -0.07112232, -0.06028879, ..., -0.05582137,\n",
      "        -0.05563801,  0.05940155],\n",
      "       [-0.05566164,  0.04676985, -0.02946233, ...,  0.05205358,\n",
      "        -0.01026798, -0.03355665],\n",
      "       [-0.06728154, -0.05630093, -0.06812032, ..., -0.03156614,\n",
      "         0.04599058, -0.06836499]], dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([-0.03263706, -0.01400839, -0.02344826, -0.02079244, -0.02892002,\n",
      "       -0.0113265 , -0.00817079, -0.05019202, -0.01608184,  0.00768249,\n",
      "       -0.02314247, -0.01997226, -0.04069498, -0.04729348,  0.0039298 ,\n",
      "       -0.0271451 , -0.00223695, -0.02988881,  0.01238696, -0.00471117,\n",
      "       -0.0246967 , -0.02715413, -0.02914258, -0.02212682, -0.0078482 ,\n",
      "       -0.02360254, -0.04515817, -0.02875669, -0.02432616,  0.00405367,\n",
      "       -0.01940483, -0.03645474, -0.02239513,  0.00042797, -0.01859244,\n",
      "       -0.03318299, -0.03849478, -0.06696162, -0.02279932, -0.03454672,\n",
      "        0.02895718, -0.05137909, -0.02079769, -0.01697245, -0.00911897,\n",
      "       -0.02454099, -0.02642206, -0.01536894, -0.02097871, -0.02158653,\n",
      "       -0.02304149, -0.00541842, -0.03778563, -0.02174171, -0.01030829,\n",
      "       -0.0165211 , -0.04502222, -0.00522189, -0.0187784 , -0.02604798,\n",
      "       -0.02061657, -0.03641075, -0.01743855, -0.01981257, -0.02016996,\n",
      "       -0.01026435, -0.03149802, -0.02054737, -0.02448671, -0.02076966,\n",
      "       -0.04258164, -0.02558228, -0.0190596 , -0.03349122, -0.0131617 ,\n",
      "       -0.03592488,  0.01295764, -0.0841026 , -0.01489619, -0.04767759,\n",
      "       -0.01812584, -0.02455795, -0.01797661, -0.03208281, -0.0332126 ,\n",
      "       -0.02130189, -0.05002311, -0.026601  , -0.02332207, -0.03255921,\n",
      "       -0.01358891, -0.0095153 , -0.01156093, -0.02582159,  0.07290854,\n",
      "       -0.03470507, -0.03867908, -0.04765642, -0.01099744, -0.01221241,\n",
      "       -0.04047491, -0.01529841, -0.01999376, -0.06268281, -0.0039246 ,\n",
      "       -0.02054891, -0.06137533, -0.02104353, -0.0387722 , -0.02225765,\n",
      "        0.00319941, -0.01395609, -0.02671243, -0.0258695 , -0.02752911,\n",
      "       -0.03111562, -0.04306994, -0.02615908, -0.03605143, -0.0267573 ,\n",
      "       -0.03916477, -0.03708255, -0.02571571, -0.03838286, -0.02604759,\n",
      "       -0.02398824, -0.01959209, -0.03084542], dtype=float32)>, <tf.Variable 'dense_1/kernel:0' shape=(128, 128) dtype=float32, numpy=\n",
      "array([[-0.11923262,  0.11263184,  0.02842987, ...,  0.09145638,\n",
      "        -0.0390246 , -0.13995402],\n",
      "       [-0.16645811, -0.00552106, -0.17118675, ..., -0.12335308,\n",
      "        -0.16956043, -0.02863881],\n",
      "       [ 0.02456912,  0.01764509, -0.13192357, ..., -0.04970521,\n",
      "        -0.08921184,  0.07326904],\n",
      "       ...,\n",
      "       [-0.05740825, -0.11734758,  0.1238673 , ..., -0.13121073,\n",
      "         0.07064776, -0.03747737],\n",
      "       [-0.0423125 , -0.10243578, -0.08272552, ..., -0.16468778,\n",
      "         0.01628302, -0.01377949],\n",
      "       [ 0.07659775,  0.05367789, -0.06430629, ...,  0.13254482,\n",
      "         0.01346748,  0.0930995 ]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([-0.13479325, -0.10167819, -0.12039891, -0.13046087, -0.01608954,\n",
      "        0.00248953,  0.00896825, -0.10318314, -0.02915695, -0.04502329,\n",
      "       -0.1479247 , -0.07205617, -0.10781058, -0.10322394, -0.13016294,\n",
      "       -0.09112601, -0.12109177, -0.04651061, -0.1180437 , -0.07547558,\n",
      "       -0.06983947, -0.07211562, -0.06973748, -0.08800974, -0.1252479 ,\n",
      "       -0.01452371, -0.03380536, -0.12111744, -0.16598271, -0.02544201,\n",
      "       -0.06179496, -0.16546208, -0.13092597, -0.10636557, -0.1836857 ,\n",
      "       -0.15142047, -0.16119885, -0.09582594, -0.06114284, -0.1668696 ,\n",
      "       -0.05756938, -0.13182741, -0.03496207, -0.03227911, -0.14067587,\n",
      "       -0.08115742, -0.1193973 , -0.0932257 , -0.17147593, -0.13867877,\n",
      "       -0.12402375, -0.03416554, -0.04667821, -0.13253191,  0.00177533,\n",
      "       -0.05918743, -0.14143914, -0.08806488, -0.11181056, -0.17529273,\n",
      "       -0.03770231, -0.04732884, -0.04594046, -0.17782861, -0.17351854,\n",
      "       -0.13048357, -0.13653179, -0.11389977, -0.06783734, -0.07516297,\n",
      "       -0.1283917 , -0.09341648, -0.07010189, -0.09638689, -0.08577272,\n",
      "       -0.10321394, -0.08361796, -0.17204791, -0.07212511, -0.12689227,\n",
      "       -0.10190535, -0.0972073 , -0.23661152,  0.03863842, -0.067773  ,\n",
      "       -0.12132997,  0.01131081, -0.04807063, -0.08411199, -0.15867242,\n",
      "       -0.05468889, -0.08490594, -0.07735146, -0.16352451, -0.09283418,\n",
      "       -0.09833224, -0.11549231, -0.13290492, -0.12563622, -0.15315562,\n",
      "       -0.10014503, -0.14399968, -0.06395046, -0.10509067, -0.13824873,\n",
      "       -0.11874676, -0.12624446, -0.13442959, -0.22672684, -0.07006115,\n",
      "       -0.01682727, -0.07763443, -0.02135221, -0.16400197, -0.12617245,\n",
      "       -0.03229372, -0.0909386 , -0.04019705, -0.10191125, -0.08372632,\n",
      "       -0.14170122, -0.0910344 , -0.17879999, -0.08231336, -0.06597155,\n",
      "       -0.02324436, -0.06739482, -0.08755258], dtype=float32)>, <tf.Variable 'dense_2/kernel:0' shape=(128, 10) dtype=float32, numpy=\n",
      "array([[-0.09476245,  0.15722924, -0.02323884, ...,  0.12293543,\n",
      "         0.10163311,  0.08633956],\n",
      "       [-0.05632696, -0.03394677, -0.05904766, ...,  0.06280237,\n",
      "         0.12656695, -0.05985105],\n",
      "       [-0.02801582, -0.04320116,  0.02122776, ..., -0.11644452,\n",
      "         0.06697353, -0.11145711],\n",
      "       ...,\n",
      "       [-0.02210399, -0.07099009,  0.08116224, ..., -0.08250734,\n",
      "         0.00323139, -0.20187412],\n",
      "       [-0.14316714,  0.118279  ,  0.23190157, ...,  0.0146833 ,\n",
      "        -0.18588218,  0.0782175 ],\n",
      "       [ 0.0138471 , -0.09994041, -0.09247477, ..., -0.03526993,\n",
      "        -0.0469265 ,  0.10412616]], dtype=float32)>, <tf.Variable 'dense_2/bias:0' shape=(10,) dtype=float32, numpy=\n",
      "array([-0.0214444 ,  0.02134986, -0.06034401, -0.01007799,  0.02656961,\n",
      "       -0.04957062, -0.08758651,  0.01730077,  0.09603078,  0.03854476],\n",
      "      dtype=float32)>]\n",
      "1875/1875 [==============================] - 2s 852us/step - loss: 0.2663 - accuracy: 0.9366\n"
     ]
    }
   ],
   "source": [
    "training_step = model.fit(x_train, y_train, epochs=3, callbacks=[myCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377678de",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can evaluate it against the test set to see how well it performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8092cf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 665us/step - loss: 0.3254 - accuracy: 0.9270\n",
      "Test accuracy: 0.9270\n"
     ]
    }
   ],
   "source": [
    "(loss, acc) = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c092ae0",
   "metadata": {},
   "source": [
    "To further prove convergance we can also plot the loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5ba0d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x217514586a0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Loss\")\n",
    "plt.plot(training_step.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96027b9",
   "metadata": {},
   "source": [
    "Since we have only 3 epochs we don't have many data points for the plot, but this could be something to change when tuning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97131925",
   "metadata": {},
   "source": [
    "### Normalizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b39c2e5",
   "metadata": {},
   "source": [
    "Let's do a common technique and normalize the data. This will  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b0cfe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm = x_train / 255\n",
    "x_test_norm = x_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e02d46",
   "metadata": {},
   "source": [
    "Now the minimum and maximum values should fall between [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c42d23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.min(x_train_norm))\n",
    "print(np.max(x_train_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a36aa80",
   "metadata": {},
   "source": [
    "Let's re-train and evaluate the model (without the callback, for cleaner output). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc5e7b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 2s 821us/step - loss: 0.4937 - accuracy: 0.8885\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 2s 807us/step - loss: 0.1297 - accuracy: 0.9618\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 2s 806us/step - loss: 0.0935 - accuracy: 0.9719\n"
     ]
    }
   ],
   "source": [
    "training_step_2 = model.fit(x_train_norm, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f6a4d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 658us/step - loss: 0.0965 - accuracy: 0.9695\n",
      "Test accuracy: 0.9695\n"
     ]
    }
   ],
   "source": [
    "(loss, acc) = model.evaluate(x_test_norm, y_test)\n",
    "print(\"Test accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a132405e",
   "metadata": {},
   "source": [
    "It looks like normalizing the data yeilded slightly better performance from the model. \n",
    "\n",
    "### Changing the Number of Epochs\n",
    "\n",
    "Another parameter we can control is the number of epochs run during the training step. Let's try to increase the number of epoch and observe the accuracy and loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8aaa5850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1875/1875 [==============================] - 2s 836us/step - loss: 0.0722 - accuracy: 0.9777\n",
      "Epoch 2/50\n",
      "1875/1875 [==============================] - 2s 838us/step - loss: 0.0594 - accuracy: 0.9807\n",
      "Epoch 3/50\n",
      "1875/1875 [==============================] - 2s 820us/step - loss: 0.0459 - accuracy: 0.9853\n",
      "Epoch 4/50\n",
      "1875/1875 [==============================] - 2s 811us/step - loss: 0.0388 - accuracy: 0.9875\n",
      "Epoch 5/50\n",
      "1875/1875 [==============================] - 2s 812us/step - loss: 0.0316 - accuracy: 0.9893\n",
      "Epoch 6/50\n",
      "1875/1875 [==============================] - 2s 815us/step - loss: 0.0273 - accuracy: 0.9910\n",
      "Epoch 7/50\n",
      "1875/1875 [==============================] - 2s 814us/step - loss: 0.0232 - accuracy: 0.9924\n",
      "Epoch 8/50\n",
      "1875/1875 [==============================] - 2s 829us/step - loss: 0.0211 - accuracy: 0.9929\n",
      "Epoch 9/50\n",
      "1875/1875 [==============================] - 2s 824us/step - loss: 0.0174 - accuracy: 0.9944\n",
      "Epoch 10/50\n",
      "1875/1875 [==============================] - 2s 817us/step - loss: 0.0158 - accuracy: 0.9948\n",
      "Epoch 11/50\n",
      "1875/1875 [==============================] - 2s 815us/step - loss: 0.0142 - accuracy: 0.9952\n",
      "Epoch 12/50\n",
      "1875/1875 [==============================] - 2s 819us/step - loss: 0.0154 - accuracy: 0.9947\n",
      "Epoch 13/50\n",
      "1875/1875 [==============================] - 2s 821us/step - loss: 0.0109 - accuracy: 0.9959\n",
      "Epoch 14/50\n",
      "1875/1875 [==============================] - 2s 828us/step - loss: 0.0110 - accuracy: 0.9965\n",
      "Epoch 15/50\n",
      "1875/1875 [==============================] - 2s 823us/step - loss: 0.0122 - accuracy: 0.9961\n",
      "Epoch 16/50\n",
      "1875/1875 [==============================] - 2s 821us/step - loss: 0.0106 - accuracy: 0.9968\n",
      "Epoch 17/50\n",
      "1875/1875 [==============================] - 2s 817us/step - loss: 0.0107 - accuracy: 0.9965\n",
      "Epoch 18/50\n",
      "1875/1875 [==============================] - 2s 816us/step - loss: 0.0105 - accuracy: 0.9966\n",
      "Epoch 19/50\n",
      "1875/1875 [==============================] - 2s 821us/step - loss: 0.0078 - accuracy: 0.9973\n",
      "Epoch 20/50\n",
      "1875/1875 [==============================] - 2s 819us/step - loss: 0.0089 - accuracy: 0.9972\n",
      "Epoch 21/50\n",
      "1875/1875 [==============================] - 2s 831us/step - loss: 0.0091 - accuracy: 0.9972\n",
      "Epoch 22/50\n",
      "1875/1875 [==============================] - 2s 822us/step - loss: 0.0082 - accuracy: 0.9972\n",
      "Epoch 23/50\n",
      "1875/1875 [==============================] - 2s 827us/step - loss: 0.0084 - accuracy: 0.9975\n",
      "Epoch 24/50\n",
      "1875/1875 [==============================] - 2s 830us/step - loss: 0.0088 - accuracy: 0.9974\n",
      "Epoch 25/50\n",
      "1875/1875 [==============================] - 2s 845us/step - loss: 0.0067 - accuracy: 0.9978\n",
      "Epoch 26/50\n",
      "1875/1875 [==============================] - 2s 830us/step - loss: 0.0068 - accuracy: 0.9977\n",
      "Epoch 27/50\n",
      "1875/1875 [==============================] - 2s 832us/step - loss: 0.0071 - accuracy: 0.9979\n",
      "Epoch 28/50\n",
      "1875/1875 [==============================] - 2s 833us/step - loss: 0.0070 - accuracy: 0.9977\n",
      "Epoch 29/50\n",
      "1875/1875 [==============================] - 2s 844us/step - loss: 0.0057 - accuracy: 0.9984\n",
      "Epoch 30/50\n",
      "1875/1875 [==============================] - 2s 837us/step - loss: 0.0055 - accuracy: 0.9982\n",
      "Epoch 31/50\n",
      "1875/1875 [==============================] - 2s 828us/step - loss: 0.0089 - accuracy: 0.9973\n",
      "Epoch 32/50\n",
      "1875/1875 [==============================] - 2s 833us/step - loss: 0.0070 - accuracy: 0.9980\n",
      "Epoch 33/50\n",
      "1875/1875 [==============================] - 2s 844us/step - loss: 0.0058 - accuracy: 0.9983\n",
      "Epoch 34/50\n",
      "1875/1875 [==============================] - 2s 846us/step - loss: 0.0072 - accuracy: 0.9977\n",
      "Epoch 35/50\n",
      "1875/1875 [==============================] - 2s 830us/step - loss: 0.0062 - accuracy: 0.9981\n",
      "Epoch 36/50\n",
      "1875/1875 [==============================] - 2s 825us/step - loss: 0.0050 - accuracy: 0.9984\n",
      "Epoch 37/50\n",
      "1875/1875 [==============================] - 2s 821us/step - loss: 0.0046 - accuracy: 0.9987\n",
      "Epoch 38/50\n",
      "1875/1875 [==============================] - 2s 838us/step - loss: 0.0066 - accuracy: 0.9980\n",
      "Epoch 39/50\n",
      "1875/1875 [==============================] - 2s 849us/step - loss: 0.0063 - accuracy: 0.9982\n",
      "Epoch 40/50\n",
      "1875/1875 [==============================] - 2s 833us/step - loss: 0.0057 - accuracy: 0.9982\n",
      "Epoch 41/50\n",
      "1875/1875 [==============================] - 2s 847us/step - loss: 0.0048 - accuracy: 0.9987\n",
      "Epoch 42/50\n",
      "1875/1875 [==============================] - 2s 828us/step - loss: 0.0075 - accuracy: 0.9980\n",
      "Epoch 43/50\n",
      "1875/1875 [==============================] - 2s 829us/step - loss: 0.0052 - accuracy: 0.9985\n",
      "Epoch 44/50\n",
      "1875/1875 [==============================] - 2s 843us/step - loss: 0.0047 - accuracy: 0.9987\n",
      "Epoch 45/50\n",
      "1875/1875 [==============================] - 2s 832us/step - loss: 0.0055 - accuracy: 0.9982\n",
      "Epoch 46/50\n",
      "1875/1875 [==============================] - 2s 828us/step - loss: 0.0050 - accuracy: 0.9987\n",
      "Epoch 47/50\n",
      "1875/1875 [==============================] - 2s 840us/step - loss: 0.0043 - accuracy: 0.9987\n",
      "Epoch 48/50\n",
      "1875/1875 [==============================] - 2s 825us/step - loss: 0.0054 - accuracy: 0.9987\n",
      "Epoch 49/50\n",
      "1875/1875 [==============================] - 2s 825us/step - loss: 0.0065 - accuracy: 0.9984\n",
      "Epoch 50/50\n",
      "1875/1875 [==============================] - 2s 826us/step - loss: 0.0063 - accuracy: 0.9984\n"
     ]
    }
   ],
   "source": [
    "training_step_3 = model.fit(x_train_norm, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c9f74e",
   "metadata": {},
   "source": [
    "Let's re-evaluate the model against the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06a4ec41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 674us/step - loss: 0.2202 - accuracy: 0.9793\n",
      "Test accuracy: 0.9793\n"
     ]
    }
   ],
   "source": [
    "(loss, acc) = model.evaluate(x_test_norm, y_test)\n",
    "print(\"Test accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e8ef6",
   "metadata": {},
   "source": [
    "Let's plot the loss as a function of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2608fa26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x217634941f0>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAnYElEQVR4nO3deXiU5b3/8fc3eyCBAFnYtxAEXECMLAWtArWgVLpatVZrbamtttrj6antqT3HLuec3znWKlWx1q1a11pt0dK6o1JlCSogewhgwpKEYEJCyP79/TEDpiGQAZJMMvN5XddcyTzPPZnvDRefPNxz3/dj7o6IiESumHAXICIiHUtBLyIS4RT0IiIRTkEvIhLhFPQiIhFOQS8iEuEU9CIiEU5BL1HNzLab2axw1yHSkRT0IiIRTkEv0oKZJZrZHWa2K/i4w8wSg+fSzewFMys3s31m9paZxQTP/dDMdppZpZltMrOZ4e2JSEBcuAsQ6YL+HZgCTAAc+AvwE+AW4CagCMgItp0CuJmdAlwPnO3uu8xsOBDbuWWLtE5X9CJH+grwM3cvcfdS4Fbgq8Fz9cAAYJi717v7Wx7YMKoRSATGmVm8u293961hqV6kBQW9yJEGAjuaPd8RPAbwf0A+8JKZFZjZzQDung/cCPwnUGJmT5rZQES6AAW9yJF2AcOaPR8aPIa7V7r7Te4+EvgM8C+HxuLd/XF3nx58rQP/r3PLFmmdgl4E4s0s6dADeAL4iZllmFk68FPgDwBmNtfMRpmZAfsJDNk0mtkpZjYj+KFtDXAweE4k7BT0IrCYQDAfeiQBecAaYC3wLvCLYNsc4BWgCngHuMfdlxAYn/8fYC+wB8gEftxpPRA5BtONR0REIpuu6EVEIpyCXkQkwinoRUQinIJeRCTCdcktENLT03348OHhLkNEpNtYtWrVXnfPaO1clwz64cOHk5eXF+4yRES6DTPbcbRzGroREYlwCnoRkQinoBcRiXAKehGRCKegFxGJcAp6EZEIp6AXEYlwERP09Y1N3LMknzc3l4a7FBGRLiVigj4uxvjtGwX8fd2ecJciItKlREzQmxmjs1LYUlwZ7lJERLqUiAl6gFGZqWwurkI3UxER+VhEBf3orBQqDtZTWlUb7lJERLqMkILezGab2SYzyzezm1s5b2a2IHh+jZlNDB4/xczeb/bYb2Y3tnMfDsvJTAUgv7iqo95CRKTbaTPozSwWuBuYA4wDLjOzcS2azSFw0+QcYD6wEMDdN7n7BHefAJwFVAPPtVv1LYzOSgFgs8bpRUQOC+WKfhKQ7+4F7l4HPAnMa9FmHvCIBywD0sxsQIs2M4Gt7n7UrTRPVkZqIr2T49lSoit6EZFDQgn6QUBhs+dFwWPH2+ZS4ImjvYmZzTezPDPLKy09sbnwZkZOZgpbNHQjInJYKEFvrRxrOa3lmG3MLAG4GPjj0d7E3e9z91x3z83IaPUmKSHJyUplc0mlZt6IiASFEvRFwJBmzwcDu46zzRzgXXcvPpEij0dOZgrl1fXsrarr6LcSEekWQgn6lUCOmY0IXplfCixq0WYRcGVw9s0UoMLddzc7fxnHGLZpT6OzAjNvtpToA1kREQgh6N29AbgeeBHYADzt7uvM7FozuzbYbDFQAOQDvwO+c+j1ZtYD+BTwbDvX3qqc4MwbjdOLiASEdHNwd19MIMybH7u32fcOXHeU11YD/U6ixuOSmZpIr6Q4XdGLiARF1MpYCM68yQpshSAiIhEY9BBYOJWvufQiIkCEBv2ozFT2Hahjr/a8ERGJzKAfrQ9kRUQOi8igP7S5mT6QFRGJ0KDP6pVIalKcNjcTESFCg1573oiIfCwigx4CK2S1i6WISAQH/ajMFPYdqKNMM29EJMpFbNAf2vNGC6dEJNpFbNAf2vMmXzNvRCTKRWzQ9++VRGpinK7oRSTqRWzQmxmjslI0l15Eol7EBj3A6MxUTbEUkagX0UGfk5VCmWbeiEiUi/CgP7QVgq7qRSR6RXbQZwY3N1PQi0gUi+igH9A7iZTEOLZozxsRiWIRHfRmxijteSMiUS6igx4Ce9NriqWIRLOQgt7MZpvZJjPLN7ObWzlvZrYgeH6NmU1sdi7NzJ4xs41mtsHMprZnB9qSk5nK3qo69h2o68y3FRHpMtoMejOLBe4G5gDjgMvMbFyLZnOAnOBjPrCw2bk7gb+7+xhgPLChHeoOWc7hu03pql5EolMoV/STgHx3L3D3OuBJYF6LNvOARzxgGZBmZgPMrBdwLvAAgLvXuXt5+5XfNk2xFJFoF0rQDwIKmz0vCh4Lpc1IoBR4yMzeM7P7zaxna29iZvPNLM/M8kpLS0PuQFsG9k6iZ0KsruhFJGqFEvTWyjEPsU0cMBFY6O5nAgeAI8b4Adz9PnfPdffcjIyMEMoKTWDPm1RtbiYiUSuUoC8ChjR7PhjYFWKbIqDI3ZcHjz9DIPg7VU5mCltLFfQiEp1CCfqVQI6ZjTCzBOBSYFGLNouAK4Ozb6YAFe6+2933AIVmdkqw3UxgfXsVH6rsjBRKKmvZX1Pf2W8tIhJ2cW01cPcGM7seeBGIBR5093Vmdm3w/L3AYuBCIB+oBq5u9iO+CzwW/CVR0OJcpxgV3Apha0kVZw7t09lvLyISVm0GPYC7LyYQ5s2P3dvseweuO8pr3wdyT7zEk5edEfj8N19BLyJRKOJXxgIM7duD+Fhja+mBcJciItLpoiLo42JjGN6vJ/maSy8iUSgqgh4C4/QFmnkjIlEoaoI+OyOFHfuqqWtoCncpIiKdKmqCflRmCo1Nzo4yjdOLSHSJmqDPzghMsdQ4vYhEm+gJ+szAFEutkBWRaBM1Qd8jIY5Bacm6oheRqBM1QQ8wMqOn5tKLSNSJqqAfFdzcrKmp5eabIiKRK6qCPjsjheq6Rvbsrwl3KSIinSaqgv7Q5mYapxeRaBJVQa8pliISjaIq6NNTEuidHK8pliISVaIq6M2M7AxtbiYi0SWqgh4OzbzRFEsRiR5RGfR7q2qpqNZtBUUkOkRd0B/+QFbj9CISJaIu6JvfP1ZEJBpEXdAP7tODhLgYzbwRkagRUtCb2Wwz22Rm+WZ2cyvnzcwWBM+vMbOJzc5tN7O1Zva+meW1Z/EnIjbGGJmumTciEj3i2mpgZrHA3cCngCJgpZktcvf1zZrNAXKCj8nAwuDXQ853973tVvVJys5IYd2uinCXISLSKUK5op8E5Lt7gbvXAU8C81q0mQc84gHLgDQzG9DOtbab7MwUPtxXTU19Y7hLERHpcKEE/SCgsNnzouCxUNs48JKZrTKz+Ud7EzObb2Z5ZpZXWloaQlknLjujJ00O23VbQRGJAqEEvbVyrOU+v8dqM83dJxIY3rnOzM5t7U3c/T53z3X33IyMjBDKOnEfz7xR0ItI5Asl6IuAIc2eDwZ2hdrG3Q99LQGeIzAUFFYj01Mw0+ZmIhIdQgn6lUCOmY0wswTgUmBRizaLgCuDs2+mABXuvtvMeppZKoCZ9QQuAD5ox/pPSHJCLIPSkjXFUkSiQpuzbty9wcyuB14EYoEH3X2dmV0bPH8vsBi4EMgHqoGrgy/PAp4zs0Pv9bi7/73de3ECRmWm6IpeRKJCm0EP4O6LCYR582P3NvvegetaeV0BMP4ka+wQ2RkpLCsoo6nJiYlp7SMGEZHIEHUrYw8ZlZlCTX0TO8sPhrsUEZEOFbVBf2hzM43Ti0iki9qg1/1jRSRaRG3Q9+2ZQJ8e8boJiYhEvKgNegjebUpX9CIS4aI66LMzUthSUklg0pCISGSK6qCfOLQPH1XXs373/nCXIiLSYaI66M8fkwnAqxtKwlyJiEjHieqgz0hNZPyQNF7dUBzuUkREOkxUBz3ArDGZrC6qoKSyJtyliIh0iKgP+pljswB4faOGb0QkMkV90I8dkMrA3km8onF6EYlQUR/0ZsaMsZks3bJXtxYUkYgU9UEPgeGbg/WNvFNQFu5SRETanYIemDqyH8nxsZp9IyIRSUEPJMXHMj0nndc2lGiVrIhEHAV90KyxmeyqqGHD7spwlyIi0q4U9EEfr5LV8I2IRBYFfVBmahLjh6TxiubTi0iEUdA3M3NMJqsLy7VKVkQiSkhBb2azzWyTmeWb2c2tnDczWxA8v8bMJrY4H2tm75nZC+1VeEeYOTYwfLNkY2mYKxERaT9tBr2ZxQJ3A3OAccBlZjauRbM5QE7wMR9Y2OL8DcCGk662g40b0Cu4Slbj9CISOUK5op8E5Lt7gbvXAU8C81q0mQc84gHLgDQzGwBgZoOBi4D727HuDnFolexbWiUrIhEklKAfBBQ2e14UPBZqmzuAfwOajvUmZjbfzPLMLK+0NHxDJzPHaJWsiESWUILeWjnWclVRq23MbC5Q4u6r2noTd7/P3XPdPTcjIyOEsjrG1OzAKtnXtMmZiESIUIK+CBjS7PlgYFeIbaYBF5vZdgJDPjPM7A8nXG0nOLRK9tUNxVolKyIRIZSgXwnkmNkIM0sALgUWtWizCLgyOPtmClDh7rvd/UfuPtjdhwdf95q7X9GeHegIM8dolayIRI64thq4e4OZXQ+8CMQCD7r7OjO7Nnj+XmAxcCGQD1QDV3dcyR1vRnCV7OubShg3sFeYqxEROTnWFYcncnNzPS8vL6w1fOY3S4mPNZ79zrSw1iEiEgozW+Xuua2d08rYo5gxJpP3Csspq6oNdykiIidFQX8UM8dm4g5vbNYqWRHp3hT0R3HawN5kpCbyqjY5E5FuTkF/FDExxvmnZPDmplLqG4+51ktEpEtT0B/DjDFZVNY2kLf9o3CXIiJywhT0xzA9J52E2Bhe26hNzkSk+1LQH0NKYhyTR/bVOL2IdGsK+jbMGJNJQekBtu89EO5SREROiIK+DYdWyb6mq3oR6aYU9G0Y1q8nozJTFPQi0m0p6EMwc0wmy7eVUVXbEO5SRESOm4I+BOePyaS+0Vm6RatkRaT7UdCH4KxhfeiVFMeruhmJiHRDCvoQxMfG8MlTMnl9UwlNTV1vt08RkWNR0IdoxpgM9lbVsXZnRbhLERE5Lgr6EH1ydCYxhhZPiUi3o6APUd+eCUwc2kfbIYhIt6OgPw4zxmbywc79FO+vCXcpIiIhU9Afh5ljsgB4fvWuMFciIhI6Bf1xOKV/KpNH9OW+NwuoqW8MdzkiIiEJKejNbLaZbTKzfDO7uZXzZmYLgufXmNnE4PEkM1thZqvNbJ2Z3dreHehsN84aTUllLU+s+DDcpYiIhKTNoDezWOBuYA4wDrjMzMa1aDYHyAk+5gMLg8drgRnuPh6YAMw2syntU3p4TM3ux+QRfVm4ZKuu6kWkWwjlin4SkO/uBe5eBzwJzGvRZh7wiAcsA9LMbEDweVWwTXzw0e1XHB26qn98ua7qRaTrCyXoBwGFzZ4XBY+F1MbMYs3sfaAEeNndl7f2JmY238zyzCyvtLRr7ykzNbsfU0b2ZeEbuqoXka4vlKC3Vo61vCo/aht3b3T3CcBgYJKZndbam7j7fe6e6+65GRkZIZQVXjfMHE2prupFpBsIJeiLgCHNng8GWs4vbLONu5cDS4DZx1tkV6SrehHpLkIJ+pVAjpmNMLME4FJgUYs2i4Arg7NvpgAV7r7bzDLMLA3AzJKBWcDG9is/vG6cFbiqf0xX9SLShbUZ9O7eAFwPvAhsAJ5293Vmdq2ZXRtsthgoAPKB3wHfCR4fALxuZmsI/MJ42d1faOc+hM2Ukf2YOrIf9+qqXkS6MHPvepNgcnNzPS8vL9xlhGR5QRlfvm8Zt8wdxzXTR4S7HBGJUma2yt1zWzunlbEnafLIfnwiux8Ll2zlYJ2u6kWk61HQt4MbZuawt6qWx5bvCHcpIiJHUNC3g8kj+zFtVOCqXjcQF5GuRkHfTv71glMoO1DHQ0u3hbsUEZF/oqBvJ2cO7cOnxmVx35sFlFfXhbscEZHDFPTt6KYLRlNV18C9bxSEuxQRkcMU9O1oTP9ezBs/kIff3kaJ7kIlIl2Egr6d3ThrNA2Nzl2v54e7FBERQEHf7oan9+SSs4fwxIoPKdxXHe5yREQU9B3hezNyMDPueGVLuEsREVHQd4T+vZO4auownnuviC3FleEuR0SinIK+g3z7vFEkx8dy+8ubw12KiEQ5BX0H6dszgW+cM5K/fbCHNUXl4S5HRKKYgr4DfeOcEaT1iOfW59drwzMRCRsFfQdKTYrn1otP5d0PP+Ka36+kuk774IhI51PQd7B5EwZx+yXjWVZQxtcfXskBbXomIp1MQd8JPnfmYH795Qms2LaPqx9aqR0uRaRTKeg7ybwJg7jz0jNZ9eFHXPXgCipr6sNdkohECQV9J/rM+IHcddmZrC4s58oHV7BfYS8inUBB38nmnD6Au78ykQ92VnDlAyuob2wKd0kiEuFCCnozm21mm8ws38xubuW8mdmC4Pk1ZjYxeHyImb1uZhvMbJ2Z3dDeHeiOPn1qf26/ZALvF5bz6Du6/aCIdKw2g97MYoG7gTnAOOAyMxvXotkcICf4mA8sDB5vAG5y97HAFOC6Vl4bleaeMYBzctK545XN7DugG5WISMcJ5Yp+EpDv7gXuXgc8Ccxr0WYe8IgHLAPSzGyAu+9293cB3L0S2AAMasf6uy0z46dzx3GgrpFfa5sEEelAoQT9IKCw2fMijgzrNtuY2XDgTGB5a29iZvPNLM/M8kpLS0Moq/vLyUrlislDeWz5Djbt0eZnItIxQgl6a+WYH08bM0sB/gTc6O77W3sTd7/P3XPdPTcjIyOEsiLDjbNGk5oUz89fWI97yz9WEZGTF0rQFwFDmj0fDOwKtY2ZxRMI+cfc/dkTLzUy9emZwPdn5bA0fy+vbCgJdzkiEoFCCfqVQI6ZjTCzBOBSYFGLNouAK4Ozb6YAFe6+28wMeADY4O63t2vlEeQrU4YxKjOFX/51PbUN2vxMRNpXm0Hv7g3A9cCLBD5Mfdrd15nZtWZ2bbDZYqAAyAd+B3wneHwa8FVghpm9H3xc2N6d6O7iY2O4Ze44tpdV8/u3t4e7HBGJMNYVx4Vzc3M9Ly8v3GV0uq8/vJKV2/bx+g/OIz0lMdzliEg3Ymar3D23tXNaGduF/PtFYzlY38ivXtoU7lJEJIIo6LuQ7IwUrvrEcJ5cWcgzq4rCXY6IRAgFfRdz46wcpozox7/+cTU3/2kNNfX6cFZETo6CvotJTYrn0Wsmcd352Ty5spDP3/M2O8oOhLssEenGFPRdUFxsDD/49Bge/FouO8sPMnfBUv7+wZ5wlyUi3ZSCvgubMSaLv35vOiMzenLtH1bxixfWa1tjETluCvoubnCfHjx97VSunDqM+5du48u/fYdd5QfDXZaIdCMK+m4gMS6Wn807jQWXncmmPZVctOAt3th87I3fVheW8+XfvsOXf/sOTU1db62EiHQeBX03cvH4gSz67nQyU5P42kMr+NVLm2hsEeK7Kw7y/afeZ97d/2BNUQXLt+3j1Y3aQ0ckminou5nsjBT+fN00vjBxML95LZ8r7l9OSWUNB2obuP2lTZx/2xL+unY33zkvm2U/msmQvsnc9Xq+dsYUiWJx4S5Ajl9yQiy3fWk8k0b05ad/+YCLFizFgJLKWuaeMYAfzh7DkL49APjWudn85M8f8M7WMj4xKj28hYtIWOiKvhu7JHcIf75uGmnJ8Qzqk8yfvj2Vuy6feDjkAb541mAyUhO5Z8nWMFYqIuGkK/pubkz/Xrz0/XMJ7Ah9pKT4WL4xfQT//beNrC4sZ/yQtM4tUETCTlf0EeBoIX/IV6YMo1dSHPcsye+kikSkK1HQR4GUxDi+Nm0EL64rZkux7k0rEm0U9FHi6k8MJzk+loUaqxeJOgr6KNGnZwKXTx7KX1bvonBf9XG9dtOeSq5//F2eWvmhpmmKdEMK+ijyjXNGEGNw35sFIbUvr67jp3/5gDl3vsnfP9jDD/+0lhuefJ+q2oYOrlRE2pOCPooM6J3MFyYO5qm8Qkoqa47arqGxiUfe2c55ty3hD8t2cMWUYSz/8Ux+8OlTeGHNLj7zm6Ws21XRiZWLyMlQ0EeZb30ym4bGJh5cuh2AxianqraBvVW1FO6r5vVNJVy0YCk//cs6xvbvxeIbzuFn806jX0oi150/iie+OYXqugY+d8/bPLpsh4ZyRLqBkG4ObmazgTuBWOB+d/+fFucteP5CoBr4mru/Gzz3IDAXKHH300IpKlpvDt5Zrn/8Xf66djfxMTHUtbLt8eA+yfzkorF8+tT+rU7dLKuq5V+eXs0bm0u56PQBfG9mDnUNTVTXNVBd1xh8NGBmZKYmktkrkczUJPr0iG9zKqiInJhj3Ry8zaA3s1hgM/ApoAhYCVzm7uubtbkQ+C6BoJ8M3Onuk4PnzgWqgEcU9F1D0UfVPPyP7cTFxpAcH0tyQgxJ8bEkxcfSKymO807JJCk+9pg/o6nJ+e2bBdzWysZqR5MQG0NGaiKjMlO47UvjyUhNbI/uiAjHDvpQVsZOAvLdvSD4w54E5gHrm7WZRyDIHVhmZmlmNsDdd7v7m2Y2/OS6IO1pcJ8e/GTuuJP6GTExxrfPy+b8MRlsLq4iOT6WHgmxJCcEvvaIj6PJnZLKWkoqayjZX3v4+8Vrd3Pd4+/y2DcmEx+r0UORjhZK0A8CCps9LyJw1d5Wm0HA7lALMbP5wHyAoUOHhvoyCbMx/Xsxpn+vo54fnt7ziGPn5mRw41Pv81+LN/Afnzm1I8sTEUL7MLa1QdWW/1cPpc0xuft97p7r7rkZGRnH81LpZj575iCunjach/6xnefeKwp3OSIRL5Qr+iJgSLPng4FdJ9BG5LAfXziW9bv286Nn1zI6K5VTB/Y+atutpVVsKa6ktKqO0spaSitr2VsV+DowLYmrp40gd1ifqPqg192jqr9yckIJ+pVAjpmNAHYClwKXt2izCLg+OH4/Gahw95CHbST6xMfGcNflE/nMb5byrUdX8fz10+nTM+Gf2mzfe4BfvbyZ51d/fM1gBv16JpCekki/lATe3lrG4rV7mDAkjW+eM5JPn5pFXBvj/u7Onv01bC4O/ALZXFzJ7ooavj59BOefktkh/W1Pv3hhPW9uKWXhFWeRnZES7nKkGwh1euWFwB0Eplc+6O6/NLNrAdz93uD0yruA2QSmV17t7nnB1z4BnAekA8XAf7j7A8d6P826iR7vF5Zzyb3vMHlkXx6+ehKxMUbx/hoWvLqFp1YWEh8bwzXTRzD7tP5kpibSt2fCPwV5dV0Df1pVxP1Lt7GjrJrBfZK5ZvoILjp9AGUH6thVfpBd5QfZWV7DrvKDFH5UTX5xFZXNVvf265lAfGwMZQdquevyiXz61P7h+KMIybKCMi69bxkxBqlJ8dz31bOYPLJfuMuSLuCkpleGg4I+ujy54kNufnYtV08bTmJcLA+/vY3GJufySUO5bsYoMlOT2vwZjU3Oy+uLuf+tAvJ2fHTE+fhYY0DvZAalJTMqM4XRWSnkZKWSk5lCv5REKg7Wc9WDK/hgZwULLjuTC08f0BFdPSk19Y1ceOdb1Dc1cf+VZ/Ptx1ZRtO8g//elM5g3YVC4y5MwU9BLl/ejZ9fyxIoPMYPPThjE92eNZmi/Hm2/sBXvfvgR731YTv9eSQxMS2JQWjLpKYnExBx7TLuypp6rH1rJe4Xl3H7J+C4Xnre9uIm7Xs/n0WsmcU5OBuXVdXzr0VUs37aPmz41mutnjNK4fRRT0EuXV9vQyO/f3s45ORmMHXD06Zod7UBtA19/eCUrt+/j/744ni+cNThstTS3cc9+5i5YysUTBnL7JRMOH69taOTmP63lufd28qWzBvPLz51OQlzoaxMqDtaTFB9DYtyxF8h1ZW/n7+U/n1/Hz+edFtXDWAp6keNQXdfANx/J4+2tZfzP50/ny2e3vq6jqcnZWX6QraVVFJQeOPy1uLKGhNgYEuJiDn9NjIuhV3I8V08bwYTjvJ1jY5PzhYVv8+G+al75l0/St8WH1u7Or1/ZwoJXtzB1ZD/+94tn/NN9g1vT0NjEwiVbufPVLaT1SOCqqcP4ypRhR/zsrm5H2QEuvusfVBysJyUxjse/OZkzBqeFu6ywUNCLHKea+kbmP7qKNzeXMml4XxqamqhrbKKu4eNH2YE6ahs+3iuoV1Ic2ZkpDOydTENTE7XN2tY1NlG4r5ryg/VcNmkoP7jglCNmGR3NQ//Yxq3Pr+fOSyccczjpmVVF3PLnD2hy59vnZXPtJ7Nb3cpiS3ElN/1xNWuKKrjw9P5U1zWyZFMpSfExfPGswVwzfSQjWlnodiKampzXNpYQHxdDdkZPBvZObnMILVQHahv4/D1vs2d/Db+7Mpeb/vg+lTUNPDV/Kqf0T22X9+gI+2vqqaiuJyM1sc2tRo6Hgl7kBNTUN/LzF9azubgyeFUe+/GVelwMfXrEk52RwsiMFEZm9KRfz4RjjpFX1tRzxytbePjt7fRKiuPmOWP40llDjhl8O8sP8qnb3+Ds4X15+Oqz2xyD31V+kP9avIEX1uxmUFoyt8z9eHO6xibngaUF3PbSZlIS4/jFZ087/KHz5uJKHnhrG8+9t5P6piZmjc3iuvNHHff/PppraGzi5mfX8syqjxfFJcXHMLxfz+CfW09OH9Sbs4f3DfmX3iFNTc53HnuXl9bv4ZGvT2Z6TjofllXzxXvfxoFnrp3KsH7t88uqveyvqef+Nwu4f+k2qusagcBtPtNTAtOF01MSGdQnmVtOcHsSBb1IF7Jxz35u+fMHrNz+EWcOTePn807jtEFHLhhzd77+8EqWb9vHS98/l8F9Qv9w+p2tZfznonVsKq5k+qh0vnnuSH7z6hbydnzEBeOy+OXnTm91U7mSyhoefWcHjy7bQXl1PZ+dMJAfzhnDgN7Jx9XH2oZGbnzyff72wR6+NzOHT2T3o6D0AAWlVRTsDQxzFe6r5tB+eGP6pzJpRN/Dj7ZmWi14dQu3v7yZn1w0lm+cM/Lw8S3FlVzy23fomRjHH6+detx1d4Sa+kYefWcH9yzJ56Pqei46fQDnjk5nb1Ude6tqA1+DiwDjY2NYfMM5J/Q+CnqRLsbdefbdnfz33zaw70Adw9N7kpWaRP/eSWT1SiKrVyLl1fXc+eoWbpk7jmumjzju92hobOIPy3Zw+8ub2V/TQK+kOG6ddyqfnTCozf8ZHKhtYOGSrdz3VgGxFtjAbv65I0MaajhY18i3/hAY9jpW7TX1jawpqmDFtjKWb9vHqh0fHb7SPW1QL66cOpyLxw884j1fWreH+Y+u4vNnDuJXl4w/oi9riyq47HfLyOqVyFPfmkp6Sui7pG7cs5+S/bXkDu9Dj4RQ1pMeXUNjE8+sKuLOV7ewu6KGc0dn8IMLTuH0wUdfBX4yFPQiXVTFwXoeWLqN/JJKivfXsqeihpLKGuobA/8uxw9J49lvf4LYkxjXLquq5bn3djL3jIH07932moTmCvdV899/28DitXsYlJbMzXPGMPeMAUf9RbG/pp5vPJzHyh37jvlBdmvqG5tYv2s/ywrKePbdnWwqrqRvzwQunzSUK6YMo3/vJLYUV/LZu/9BdmYKT39r6lF/8azYto8rH1zOyPTAlthj+qcedYistqGRv63dw6PLdrAquAYjITaG3OF9OCcng3Ny0hk3oFfIny00NDaxaPUu7notn4K9BzhzaBr/9ukxTM3u2BlBCnqRbqSpyfmouo7i/bUM7ptMr6T4cJfEsoIybn1+PRt272fsgF5MHtGXMwb35ozBaYxM70lMjLHvQB1XPbiCDbv38+svT+Az4wee8Pu5O+9sLeOht7fzyoZiYsyYfVp/1u2soKq2kee/O63NYZk3Npfyzd/nUdfYRJ8e8Uwe0Y8pI/syNTudnMwUdpYf5PEVH/LUykL2HahjRHpPvjJ5KKMyU3h7axlvbi5l455KILB6enpOOjPGZPLJ0Rmk9TjyM4X6xiaee3cndy/JZ0dZNWMH9OLGWTlcMC6rU9Y3KOhF5KQ1Njl/zCvk2Xd3snZnBQfrA8MsqYlxnDaoN8X7a9hZfpCFV0xkxpisdnvfD8uqeXTZdp5cWUhNfSNPfHMKucP7hvTaPRU1LM3fy7KCMt7ZWsbO8oMApPWIp+JgPQbMGpvFV6cOY1p2+hFX7SX7A69/a8te3txcStmBOmIMzhrWhxljspgxJpPh6T14ZlUR97y+lZ3lBzl9UG++NzOHWWMzO3UBm4JeRNpVQ2MT+aVVrCmsYHVROWuKKig/WMf/fmF8hw1RHKht4KPquuP6ULqlwn3VLCsoY+X2ffTvncylZw9hYFpoH9g2NTmri8p5bWMJr20sYd2u/QAkxMVQ19DEhCFp3DAzh/NOyQjLCmUFvYhIO9tdcZDXN5aydmdgPcL0Uelh3YLiZG8lKCIiLQzonczlk7vH3fB0w04RkQinoBcRiXAKehGRCKegFxGJcAp6EZEIp6AXEYlwCnoRkQinoBcRiXBdcmWsmZUCO07w5enA3nYsp7tQv6OL+h1dQun3MHfPaO1Elwz6k2FmeUdbBhzJ1O/oon5Hl5Ptt4ZuREQinIJeRCTCRWLQ3xfuAsJE/Y4u6nd0Oal+R9wYvYiI/LNIvKIXEZFmFPQiIhEuYoLezGab2SYzyzezm8NdT0cyswfNrMTMPmh2rK+ZvWxmW4Jf+4SzxvZmZkPM7HUz22Bm68zshuDxSO93kpmtMLPVwX7fGjwe0f0+xMxizew9M3sh+Dxa+r3dzNaa2ftmlhc8dsJ9j4igN7NY4G5gDjAOuMzMxoW3qg71MDC7xbGbgVfdPQd4Nfg8kjQAN7n7WGAKcF3w7zjS+10LzHD38cAEYLaZTSHy+33IDcCGZs+jpd8A57v7hGbz50+47xER9MAkIN/dC9y9DngSmBfmmjqMu78J7GtxeB7w++D3vwc+25k1dTR33+3u7wa/ryTwj38Qkd9vd/eq4NP44MOJ8H4DmNlg4CLg/maHI77fx3DCfY+UoB8EFDZ7XhQ8Fk2y3H03BEIRyAxzPR3GzIYDZwLLiYJ+B4cv3gdKgJfdPSr6DdwB/BvQ1OxYNPQbAr/MXzKzVWY2P3jshPseKTcHb+3W65o3GoHMLAX4E3Cju+83a+2vPrK4eyMwwczSgOfM7LQwl9ThzGwuUOLuq8zsvDCXEw7T3H2XmWUCL5vZxpP5YZFyRV8EDGn2fDCwK0y1hEuxmQ0ACH4tCXM97c7M4gmE/GPu/mzwcMT3+xB3LweWEPh8JtL7PQ242My2ExiKnWFmfyDy+w2Au+8Kfi0BniMwPH3CfY+UoF8J5JjZCDNLAC4FFoW5ps62CLgq+P1VwF/CWEu7s8Cl+wPABne/vdmpSO93RvBKHjNLBmYBG4nwfrv7j9x9sLsPJ/Dv+TV3v4II7zeAmfU0s9RD3wMXAB9wEn2PmJWxZnYhgTG9WOBBd/9leCvqOGb2BHAega1Li4H/AP4MPA0MBT4EvuTuLT+w7bbMbDrwFrCWj8dsf0xgnD6S+30GgQ/eYglcmD3t7j8zs35EcL+bCw7d/Ku7z42GfpvZSAJX8RAYXn/c3X95Mn2PmKAXEZHWRcrQjYiIHIWCXkQkwinoRUQinIJeRCTCKehFRCKcgl5EJMIp6EVEItz/B2bNZHabglxHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Loss\")\n",
    "plt.plot(training_step_3.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb6cc01",
   "metadata": {},
   "source": [
    "Clearly 50 epochs is excessive, as it yeilds minimal performance increase, while taking a lot of extra time. it each epoch takes 2 seconds, the whole training step took roughly 100 seconds (94 more seconds than the first two training steps). Comapring the results, we certainly observe better accuracy, but the accuracy increase between the previous training steps and this one is small (about 1%).\n",
    "\n",
    "By observing the loss plot, we can observe the asymptotic behavior, which is good. It seems to plateau around 20 or so epochs. However, considering how accurate the model already is with just three epochs, there's certainly a strong argument that the increase in execution time for an increased number of epochs is simply not worth the benefits. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bacda16",
   "metadata": {},
   "source": [
    "### Learning Rate Tuning\n",
    "\n",
    "Let's expiriment with a few different learning rates for the ADAM optimizer. I'll also go back to using just 3 epochs for the training step. Since we're changing the hyperparameter for the gradient descent algorithm, we'll have to recompile the model with the new hyperparameter. \n",
    "\n",
    "first lets try a higher learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e55da9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    # layer to take input and flatten the shape\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    \n",
    "    # two dense hidden layers with ReLU activation function\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    \n",
    "    # final layer of shape (,10) \n",
    "    keras.layers.Dense(10)  \n",
    "    \n",
    "])\n",
    "loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optim = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optim, loss=loss_function, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "035c4848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 2s 838us/step - loss: 0.2726 - accuracy: 0.9204\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 1s 796us/step - loss: 0.1893 - accuracy: 0.9506\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 1s 796us/step - loss: 0.1564 - accuracy: 0.9582\n",
      "313/313 [==============================] - 0s 636us/step - loss: 0.1894 - accuracy: 0.9513\n",
      "Test accuracy: 0.9513\n"
     ]
    }
   ],
   "source": [
    "training_step_4 = model.fit(x_train_norm, y_train, epochs=3)\n",
    "(loss, acc) = model.evaluate(x_test_norm, y_test)\n",
    "print(\"Test accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ea61ac",
   "metadata": {},
   "source": [
    "A learning rate of 1e-2 seems like a decent number, but it results in a slightly lower accuracy and higher loss than our bnase value of 1e-3.\n",
    "\n",
    "Let's try and go with a lower learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f207be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    # layer to take input and flatten the shape\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    \n",
    "    # two dense hidden layers with ReLU activation function\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    \n",
    "    # final layer of shape (,10) \n",
    "    keras.layers.Dense(10)  \n",
    "    \n",
    "])\n",
    "loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optim = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optim, loss=loss_function, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37c2c0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 2s 797us/step - loss: 0.5319 - accuracy: 0.8598\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 2s 950us/step - loss: 0.2373 - accuracy: 0.9332\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 2s 975us/step - loss: 0.1840 - accuracy: 0.9474\n",
      "313/313 [==============================] - 0s 633us/step - loss: 0.1628 - accuracy: 0.9531\n",
      "Test accuracy: 0.9531\n"
     ]
    }
   ],
   "source": [
    "training_step_4 = model.fit(x_train_norm, y_train, epochs=3)\n",
    "(loss, acc) = model.evaluate(x_test_norm, y_test)\n",
    "print(\"Test accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518f60d7",
   "metadata": {},
   "source": [
    "We observe similar results, the accuracy is slightly lower and the loss is higher. Since we are trying to achieve the opposite, a learning rate an order of magnitude higher or lower than 1e-3 does not seem like a good choice. \n",
    "\n",
    "Let's try a few more that are closer to 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f7cd0",
   "metadata": {},
   "source": [
    "**learning rate = 0.0075**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0090e018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 2s 871us/step - loss: 0.2512 - accuracy: 0.9266\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 2s 848us/step - loss: 0.1579 - accuracy: 0.9551\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 2s 845us/step - loss: 0.1441 - accuracy: 0.9603\n",
      "313/313 [==============================] - 0s 665us/step - loss: 0.1754 - accuracy: 0.9515\n",
      "Test accuracy: 0.9515\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    # layer to take input and flatten the shape\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    \n",
    "    # two dense hidden layers with ReLU activation function\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    \n",
    "    # final layer of shape (,10) \n",
    "    keras.layers.Dense(10)  \n",
    "    \n",
    "])\n",
    "loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optim = keras.optimizers.Adam(learning_rate=0.0075)\n",
    "model.compile(optimizer=optim, loss=loss_function, metrics=[\"accuracy\"])\n",
    "\n",
    "training_step_4 = model.fit(x_train_norm, y_train, epochs=3)\n",
    "(loss, acc) = model.evaluate(x_test_norm, y_test)\n",
    "print(\"Test accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8d913c",
   "metadata": {},
   "source": [
    "**learning rate = 0.00075**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "13765619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 2s 830us/step - loss: 0.2550 - accuracy: 0.9252\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 2s 831us/step - loss: 0.1074 - accuracy: 0.9676\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 2s 822us/step - loss: 0.0719 - accuracy: 0.9776\n",
      "313/313 [==============================] - 0s 646us/step - loss: 0.0837 - accuracy: 0.9735\n",
      "Test accuracy: 0.9735\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    # layer to take input and flatten the shape\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    \n",
    "    # two dense hidden layers with ReLU activation function\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    \n",
    "    # final layer of shape (,10) \n",
    "    keras.layers.Dense(10)  \n",
    "    \n",
    "])\n",
    "loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optim = keras.optimizers.Adam(learning_rate=0.00075)\n",
    "model.compile(optimizer=optim, loss=loss_function, metrics=[\"accuracy\"])\n",
    "\n",
    "training_step_4 = model.fit(x_train_norm, y_train, epochs=3)\n",
    "(loss, acc) = model.evaluate(x_test_norm, y_test)\n",
    "print(\"Test accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb68719",
   "metadata": {},
   "source": [
    "**learning rate = 0.0009**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "53d0cc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 2s 831us/step - loss: 0.2399 - accuracy: 0.9300\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 2s 823us/step - loss: 0.1017 - accuracy: 0.9691\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 2s 831us/step - loss: 0.0692 - accuracy: 0.9788\n",
      "313/313 [==============================] - 0s 646us/step - loss: 0.0869 - accuracy: 0.9736\n",
      "Test accuracy: 0.9736\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    # layer to take input and flatten the shape\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    \n",
    "    # two dense hidden layers with ReLU activation function\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    \n",
    "    # final layer of shape (,10) \n",
    "    keras.layers.Dense(10)  \n",
    "    \n",
    "])\n",
    "loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optim = keras.optimizers.Adam(learning_rate=0.0009)\n",
    "model.compile(optimizer=optim, loss=loss_function, metrics=[\"accuracy\"])\n",
    "\n",
    "training_step_4 = model.fit(x_train_norm, y_train, epochs=3)\n",
    "(loss, acc) = model.evaluate(x_test_norm, y_test)\n",
    "print(\"Test accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf45367",
   "metadata": {},
   "source": [
    "Based off these results, it seems like an ideal learning rate for the optimized gradient descent is a value just below 1e-3. Changing the learning rate doesn't seem to have a large impact in this case, but it the results suggest that a value near the range of 0.00075 - 0.0009 is the best. Both offer nearly the same accuracy. The original model with a learning rate of 0.001 also performs very well, but it seems clear that it is not the best choice for a learnin rate. \n",
    "\n",
    "The loss of the original model is a tad higher, and the accuracy a tad lower. Since our goal is to minimize loss and maximize accuracy, we can safely assume we have found a more ideal range of possible learning rates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4def833",
   "metadata": {},
   "source": [
    "# Task 3: Rationale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f61a29",
   "metadata": {},
   "source": [
    "**Learning Rate Choice:** My original choice for a learning rate was 1e-3. Based off my own expoerience and other online examples, 1e-3 seemed like a good strating point for the learnin rate. It was neither too high or too low. As shown in Task2, the initial learning rate did not end up being the most ideal learning rate. A way to find the most ideal learnin rate is to try various values and observe the metric and parameters. I went through that process in Task 2, trying various higher and lower learnin rates. In the end, I setteled for a range of learning rates between [0.00075 and 0.0009] Both learning rates offer similar accuracy and loss, and both outperform the initial learnin rate of 0.001\n",
    "\n",
    "I did consider adding learning rate decay to the optimizer, but I decided that it would not be neccesary for this specific project. Learning rate decay helps to prevent divergance in loss at a certain point. Since I only used 3 epochs for the training step, a decay is likely to have little effect. Decay could certainly prove useful in other use cases however.\n",
    "\n",
    "**Epoch Choice:** My original model used just three epochs in the training step. I eventually tried a training step with more epochs to observe the behavior of the loss and to see how much it would impact accuracy. I used 50 epochs, which yeilded better accuracy, but also greatly increased execution time. The dataset is relatively large (60k data points), so each epoch will take some time. In the end, I decided that 3 epochs was fine for the model, but I also mention that slighly higher values are also acceptable. In the end, one must consider the tradeoff between the performance gain and execution time. I decided that the performane gain was too minimal to justify the execution time of higher epoch values. \n",
    "\n",
    "**Normalization:** Normalization is a common technique used in Machine Learning. In the case of this problem, normalizing the data maps elements from a range of [0,255] to [0,1]. This ended up being a very imporant decision, as the model performed noticably better with the normalized data points. \n",
    "\n",
    "**Model Structure:** I built a simple model structure for this problem. For the API, I used keras.Sequential, which is a simple and effective API that linearly stacks layers in the network. I first used a Flatten layer which transforms the input layer from (28,28) to (,28 * 28) or (,784). I then used 2 Dense layers with 128 units. This could be another paramater to modify, but it seems like optimal values lie between 32 and 512. I just chose 128. the final layer is a Dense layer with 10 units, since we are predicting 10 possible class labels. I did not have a Soltmax Activation function in the model, but instead used the \"from_logits\" argument in the loss function, which produces the same effect. \n",
    "\n",
    "**Regularization/Optimization:** I did not use Regularization in this project. I already use a low learning rate for the gradient descent, and the model is not too complex so Regularization would'nt provide much benefit. I chose the ADAM optimized algoorithm for graident descent. ADAM is both effecive and efficient, making it a great choice for many projects. there may be other option that might generalize better, but the ADAM optimizer serves well in this project. In image classification it's important for a model to be able to generalize, but ADAM can egenralize well. I also use a low learning rate. This means slower convergance but better generalization. ADAM works well with this, since ADAM offers fast convergance (This can be seen in the graphs of the loss). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4a377e",
   "metadata": {},
   "source": [
    "# Task4: Another Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067f74d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
